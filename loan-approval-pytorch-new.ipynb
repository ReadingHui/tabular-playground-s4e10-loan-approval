{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0awsat54Op_J"
      },
      "outputs": [],
      "source": [
        "# Google Colab preparation\n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip install scikit-optimize\n",
        "!pip install imblearn\n",
        "\n",
        "!git clone https://github.com/ReadingHui/tabular-playground-s4e10-loan-approval\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/tabular-playground-s4e10-loan-approval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M9PSTkQOeLX",
        "outputId": "6e25b89c-806e-4619-88e9-78c85e105a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocess\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data as data_utils\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dj3Al85OeLY"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XYxLDkedOeLY"
      },
      "outputs": [],
      "source": [
        "# In Google Colab\n",
        "train_path = '/content/tabular-playground-s4e10-loan-approval/train.csv'\n",
        "test_path = '/content/tabular-playground-s4e10-loan-approval/test.csv'\n",
        "\n",
        "# # In Local machine\n",
        "# train_path = 'train.csv'\n",
        "# test_path = 'test.csv'\n",
        "\n",
        "random_state = 1048576\n",
        "target = 'loan_status'\n",
        "cat_features = [\n",
        "    'person_home_ownership',\n",
        "    'loan_intent',\n",
        "    'loan_grade',\n",
        "    'cb_person_default_on_file'\n",
        "]\n",
        "\n",
        "num_features = [\n",
        "    'person_age',\n",
        "    'person_income',\n",
        "    'person_emp_length',\n",
        "    'loan_amnt',\n",
        "    'loan_int_rate',\n",
        "    'loan_percent_income',\n",
        "    'cb_person_cred_hist_length'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ-AkjE4OeLZ"
      },
      "source": [
        "# Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y66xYo0EOeLZ",
        "outputId": "b4154743-e98d-478b-af31-6954da984bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train is: (52780, 11); shape of y_train is: (52780,)\n",
            "Shape of X_test is: (5865, 11); shape of y_test is: (5865,)\n",
            "New shape of X_train is: (52639, 11); New shape of y_train is: (52639,)\n",
            "Target distribution before oversampling: (0: 14.22%, 1: 85.78%)\n",
            "X_train shape after oversampling: (90312, 26); y_train shape after oversampling: (90312, 26)\n",
            "Target distribution after oversampling: (0: 50.0%, 1: 50.0%)\n",
            "Shape of X_train is: (81280, 26); shape of y_train is: (81280,)\n",
            "Shape of X_val is: (9032, 26); shape of y_test is: (9032,)\n"
          ]
        }
      ],
      "source": [
        "# Data Loading\n",
        "X_train, X_test, y_train, y_test = preprocess.DataImport.get_train_test(\n",
        "    train_path, index_col=0, target=target,\n",
        "    random_state=random_state, verbose=1\n",
        "    )\n",
        "\n",
        "# Remove outliers\n",
        "X_train, y_train = preprocess.Outliers.remove_outliers(\n",
        "    X_train, y_train, verbose=1\n",
        "    )\n",
        "\n",
        "# One-Hot Encode\n",
        "encoder = preprocess.CategoryEncoder(cat_features=cat_features,\n",
        "                                     method='one_hot')\n",
        "X_train = encoder.fit_transform(X_train)\n",
        "X_test = encoder.transform(X_test)\n",
        "\n",
        "# Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "features = X_train.columns\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns=features)\n",
        "\n",
        "# Oversampling\n",
        "print(f'Target distribution before oversampling: ',\n",
        "      f'(0: {round(y_train.sum() / y_train.size * 100, 2)}%, ',\n",
        "      f'1: {round((1 - y_train.sum() / y_train.size) * 100, 2)}%)')\n",
        "oversampler = RandomOverSampler(random_state=random_state)\n",
        "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
        "print(f'X_train shape after oversampling: {X_train.shape}; ',\n",
        "      f'y_train shape after oversampling: {X_train.shape}')\n",
        "print(f'Target distribution after oversampling: ',\n",
        "      f'(0: {round(y_train.sum() / y_train.size * 100, 2)}%, ',\n",
        "      f'1: {round((1 - y_train.sum() / y_train.size) * 100, 2)}%)')\n",
        "\n",
        "# Train - Validation Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.1, random_state=random_state)\n",
        "\n",
        "print(f'Shape of X_train is: {X_train.shape}; ',\n",
        "      f'shape of y_train is: {y_train.shape}')\n",
        "print(f'Shape of X_val is: {X_val.shape}; ',\n",
        "      f'shape of y_test is: {y_val.shape}')\n",
        "\n",
        "# Casting everything to pytorch tensor\n",
        "X_train = torch.tensor(X_train.values, device=device, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, device=device, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val.values, device=device, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val.values, device=device, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test.values, device=device, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.values, device=device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sTNxLbAOeLZ"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g_1S_qlEOeLZ"
      },
      "outputs": [],
      "source": [
        "# Define the neural network model\n",
        "class BinaryClassificationNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassificationNN, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(in_features=26, out_features=32)\n",
        "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
        "        self.fc3 = nn.Linear(in_features=16, out_features=8)\n",
        "        self.fc4 = nn.Linear(in_features=8, out_features=1)\n",
        "\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Sigmoid for probability output\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()  # Custom weight initialization\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights with Xavier (Glorot) initialization\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        nn.init.xavier_uniform_(self.fc4.weight)\n",
        "\n",
        "        # Optionally, initialize biases to zero\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "        nn.init.zeros_(self.fc3.bias)\n",
        "        nn.init.zeros_(self.fc4.bias)\n",
        "\n",
        "\n",
        "    def forward(self, X: pd.DataFrame | np.ndarray) -> pd.DataFrame | np.ndarray:\n",
        "        X = self.relu(self.fc1(X))\n",
        "        X = self.relu(self.fc2(X))\n",
        "        X = self.relu(self.fc3(X))\n",
        "        X = self.sigmoid(self.fc4(X))  # Sigmoid for binary classification\n",
        "        return X\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n9InRN3ruI0"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model: nn.Module, optimizer: optim.Optimizer,\n",
        "                    epoch: int, loss: float, score: float,\n",
        "                    filename: str = 'checkpoint.pth', verbose: bool = False):\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'score': score\n",
        "    }, filename)\n",
        "    if verbose:\n",
        "        print(f'Checkpoint saved at epoch {epoch + 1}')\n",
        "\n",
        "def load_checkpoint(filename: str = 'checkpoint.pth'):\n",
        "    model = BinaryClassificationNN()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    checkpoint = torch.load(filename, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return model, optimizer, epoch, loss"
      ],
      "metadata": {
        "id": "oWQHaXe65YxP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 5, method: str = 'loss'):\n",
        "        self.patience = patience\n",
        "        self.method = method\n",
        "        self.counter = 0\n",
        "        self.best_value = np.inf\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_value):\n",
        "        if self.method == 'loss':\n",
        "            val_value = val_value\n",
        "        elif self.method == 'score':\n",
        "            val_value = -val_value\n",
        "        if val_value < self.best_value:\n",
        "            self.best_value = val_value\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "LlyWVmHK50iD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5f9aRW7PruI0"
      },
      "outputs": [],
      "source": [
        "def train_model(model: nn.Module,\n",
        "                X: torch.Tensor, y: torch.Tensor,\n",
        "                eval_set: tuple[torch.Tensor] = None,\n",
        "                num_epochs: int = 10, batch_size: int = 32,\n",
        "                shuffle: bool = True, learning_rate: float = 0.01,\n",
        "                weight_decay: float = 1e-3,\n",
        "                criterion: torch.nn.modules.loss = nn.BCELoss(),\n",
        "                early_stopping_method: str = 'loss',\n",
        "                early_stopping_patience: int = 5,\n",
        "                save_path: str = None,\n",
        "                verbose: int = 1):\n",
        "    # Initialize optimizer, early_stopping and history parameters\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "    early_stopping = EarlyStopping(patience=early_stopping_patience,\n",
        "                                   method=early_stopping_method)\n",
        "    best_loss = np.inf\n",
        "\n",
        "    losses = {\"train\": [], \"val\": []}\n",
        "    scores = {\"train\": [], \"val\": []}\n",
        "\n",
        "    # Initialize data_laoder\n",
        "    X = X.clone()\n",
        "    y = y.clone()\n",
        "    y_train_labels = y.cpu() if next(model.parameters()).is_cuda else y\n",
        "    train_tensor = data_utils.TensorDataset(X, y)\n",
        "    train_loader = data_utils.DataLoader(dataset=train_tensor,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=shuffle)\n",
        "    if eval_set:\n",
        "        X_val = eval_set[0].clone()\n",
        "        y_val = eval_set[1].clone()\n",
        "        y_val_labels = y_val.cpu() if next(model.parameters()).is_cuda \\\n",
        "        else y_val\n",
        "        val_tensor = data_utils.TensorDataset(X_val, y_val)\n",
        "        val_loader = data_utils.DataLoader(dataset=val_tensor,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=shuffle)\n",
        "\n",
        "    for epoch in range(num_epochs):  # Training loop\n",
        "            # Training phrase\n",
        "            total_train_loss = 0\n",
        "\n",
        "            model.train()  # Set the model to training mode\n",
        "            for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                outputs = torch.squeeze(outputs)\n",
        "\n",
        "                # Compute the loss\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backpropagation and optimization\n",
        "                optimizer.zero_grad()  # Clear previous gradients\n",
        "                loss.backward() # Backpropagate\n",
        "                optimizer.step() # Update weights\n",
        "\n",
        "                # Rolling sum of training loss\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                # If large verbose print batch loss\n",
        "                if verbose > 1:\n",
        "                    print_freq = num_epochs // 10\n",
        "                    if batch_idx % print_freq == 0:\n",
        "                        print(f\"Epoch [{epoch+1}/{num_epochs}], \\\n",
        "                        Batch [{batch_idx}], \\\n",
        "                        Loss: {loss.item():.4f}\") # Printing loss\n",
        "\n",
        "            # Store the loss value for this epoch\n",
        "            average_train_loss = round(total_train_loss / len(train_loader), 4)\n",
        "            losses['train'].append(average_train_loss)\n",
        "\n",
        "            # Evaluate training score\n",
        "            with torch.no_grad():\n",
        "                y_train_pred = model(X).detach()\n",
        "                if next(model.parameters()).is_cuda:\n",
        "                    y_train_pred = y_train_pred.cpu()\n",
        "                y_train_pred = y_train_pred.numpy()\n",
        "            train_score = round(roc_auc_score(y_train_labels, y_train_pred), 4)\n",
        "            scores['train'].append(train_score)\n",
        "            if verbose > 0 and not eval_set:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], \",\n",
        "                    f\"Average Train Loss: {average_train_loss}, \",\n",
        "                    f\"Training Score: {train_score}.\") # Printing loss\n",
        "\n",
        "            if eval_set:\n",
        "                total_val_loss = 0 # Initiate total_val loss\n",
        "\n",
        "                model.eval()  # Set the model to evaluation mode\n",
        "                # Evaluate the model on the validation set\n",
        "                for inputs, labels in val_loader: # For each batch\n",
        "                # Forward pass\n",
        "                    outputs = model(inputs)\n",
        "                    outputs = torch.squeeze(outputs)\n",
        "\n",
        "                    # Compute the loss\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    total_val_loss += loss.item() # Rolling sum of training loss\n",
        "\n",
        "                # Store the loss value for this epoch\n",
        "                average_val_loss = round(total_val_loss / len(val_loader), 4)\n",
        "                losses['val'].append(average_val_loss)\n",
        "\n",
        "                # Evaluate validation score\n",
        "                with torch.no_grad():\n",
        "                    y_val_pred = model(X_val).detach()\n",
        "                    if next(model.parameters()).is_cuda:\n",
        "                        y_val_pred = y_val_pred.cpu()\n",
        "                    y_val_pred = y_val_pred.numpy()\n",
        "                val_score = round(roc_auc_score(y_val_labels, y_val_pred), 4)\n",
        "                scores['val'].append(val_score)\n",
        "\n",
        "                if verbose > 0:\n",
        "                    print(f\"Epoch [{epoch+1}/{num_epochs}], \",\n",
        "                    f\"Average Train Loss: {average_train_loss}, \",\n",
        "                    f\"Training Score: {train_score}, \",\n",
        "                    f\"Average Val Loss: {average_val_loss}, \",\n",
        "                    f\"Validation Score: {val_score}.\") # Printing loss\n",
        "\n",
        "            # Update best model\n",
        "            if eval_set:\n",
        "                compare_set = 'val'\n",
        "            else:\n",
        "                compare_set = 'train'\n",
        "            if early_stopping_method == 'loss':\n",
        "                compare_val = losses[compare_set][-1]\n",
        "            elif early_stopping_method == 'score':\n",
        "                compare_val = - scores[compare_set][-1]\n",
        "            if compare_val < best_loss:\n",
        "                best_loss = average_val_loss\n",
        "                save_checkpoint(model, optimizer, epoch,\n",
        "                                losses[compare_set][-1],\n",
        "                                scores[compare_set][-1],\n",
        "                                save_path, verbose)\n",
        "\n",
        "\n",
        "            # Early Stopping\n",
        "            if early_stopping_method == 'loss':\n",
        "                early_stopping(average_val_loss)\n",
        "            elif early_stopping_method == 'score':\n",
        "                early_stopping(val_score)\n",
        "            if early_stopping.early_stop:\n",
        "                print(f\"Early stopping triggered, \",\n",
        "                      f\"validation {early_stopping_method} not improved \",\n",
        "                      f\"after {early_stopping_patience} epochs\")\n",
        "                break\n",
        "    # Pack history\n",
        "    history = {}\n",
        "    history['loss'] = losses\n",
        "    history['scores'] = scores\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wADiH_TsruI0",
        "outputId": "7bdfd76d-8698-456b-8cad-68179218a68c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000],  Average Train Loss: 0.361,  Training Score: 0.9294,  Average Val Loss: 0.3427,  Validation Score: 0.9239.\n",
            "Checkpoint saved at epoch 1\n",
            "Epoch [2/1000],  Average Train Loss: 0.3248,  Training Score: 0.9331,  Average Val Loss: 0.3303,  Validation Score: 0.9279.\n",
            "Checkpoint saved at epoch 2\n",
            "Epoch [3/1000],  Average Train Loss: 0.3185,  Training Score: 0.9345,  Average Val Loss: 0.3263,  Validation Score: 0.929.\n",
            "Checkpoint saved at epoch 3\n",
            "Epoch [4/1000],  Average Train Loss: 0.3152,  Training Score: 0.9352,  Average Val Loss: 0.3244,  Validation Score: 0.9298.\n",
            "Checkpoint saved at epoch 4\n",
            "Epoch [5/1000],  Average Train Loss: 0.3126,  Training Score: 0.9358,  Average Val Loss: 0.3229,  Validation Score: 0.9296.\n",
            "Checkpoint saved at epoch 5\n",
            "Epoch [6/1000],  Average Train Loss: 0.311,  Training Score: 0.9372,  Average Val Loss: 0.3218,  Validation Score: 0.9315.\n",
            "Checkpoint saved at epoch 6\n",
            "Epoch [7/1000],  Average Train Loss: 0.3092,  Training Score: 0.9374,  Average Val Loss: 0.3211,  Validation Score: 0.9308.\n",
            "Checkpoint saved at epoch 7\n",
            "Epoch [8/1000],  Average Train Loss: 0.3081,  Training Score: 0.9376,  Average Val Loss: 0.3221,  Validation Score: 0.9313.\n",
            "Epoch [9/1000],  Average Train Loss: 0.3068,  Training Score: 0.9376,  Average Val Loss: 0.322,  Validation Score: 0.9315.\n",
            "Epoch [10/1000],  Average Train Loss: 0.3063,  Training Score: 0.939,  Average Val Loss: 0.3182,  Validation Score: 0.9326.\n",
            "Checkpoint saved at epoch 10\n",
            "Epoch [11/1000],  Average Train Loss: 0.3047,  Training Score: 0.9395,  Average Val Loss: 0.3165,  Validation Score: 0.9323.\n",
            "Checkpoint saved at epoch 11\n",
            "Epoch [12/1000],  Average Train Loss: 0.3051,  Training Score: 0.9384,  Average Val Loss: 0.3185,  Validation Score: 0.9321.\n",
            "Epoch [13/1000],  Average Train Loss: 0.3042,  Training Score: 0.9398,  Average Val Loss: 0.3169,  Validation Score: 0.9334.\n",
            "Epoch [14/1000],  Average Train Loss: 0.3033,  Training Score: 0.9387,  Average Val Loss: 0.3154,  Validation Score: 0.9335.\n",
            "Checkpoint saved at epoch 14\n",
            "Epoch [15/1000],  Average Train Loss: 0.3028,  Training Score: 0.9398,  Average Val Loss: 0.314,  Validation Score: 0.9336.\n",
            "Checkpoint saved at epoch 15\n",
            "Epoch [16/1000],  Average Train Loss: 0.3024,  Training Score: 0.9401,  Average Val Loss: 0.3198,  Validation Score: 0.9338.\n",
            "Epoch [17/1000],  Average Train Loss: 0.302,  Training Score: 0.9405,  Average Val Loss: 0.3125,  Validation Score: 0.9344.\n",
            "Checkpoint saved at epoch 17\n",
            "Epoch [18/1000],  Average Train Loss: 0.301,  Training Score: 0.9405,  Average Val Loss: 0.3205,  Validation Score: 0.9323.\n",
            "Epoch [19/1000],  Average Train Loss: 0.3006,  Training Score: 0.9405,  Average Val Loss: 0.3152,  Validation Score: 0.933.\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "torch.manual_seed(random_state)\n",
        "model = BinaryClassificationNN()\n",
        "model = model.to(device)\n",
        "history = train_model(model,\n",
        "                      X_train, y_train,\n",
        "                       (X_val, y_val),\n",
        "                      num_epochs,\n",
        "                      learning_rate=0.001,\n",
        "                      save_path='PyTorch_best_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYUhE9YfruI1"
      },
      "source": [
        "# Loss analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbFSA2XjruI1"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history, loss: bool = True, score: bool = False):\n",
        "    num_epochs = len(history['loss']['train'])\n",
        "    if not loss and not score:\n",
        "        raise ValueError('Cannot have both loss and score set as False')\n",
        "\n",
        "    if loss:\n",
        "        plt.plot(range(1, num_epochs+1), history['loss']['train'], marker='o')\n",
        "        plt.plot(range(1, num_epochs+1), history['loss']['val'], marker='o')\n",
        "    if score:\n",
        "        plt.plot(range(1, num_epochs+1), history['scores']['train'], marker='x')\n",
        "        plt.plot(range(1, num_epochs+1), history['scores']['val'], marker='x')\n",
        "\n",
        "    if loss and score:\n",
        "        plt.title('Training Loss and Scores Over Epochs')\n",
        "    elif score:\n",
        "        plt.title('Scores Over Epochs')\n",
        "    elif loss:\n",
        "        plt.title('Training Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPLueZGFruI1"
      },
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, optimizer, epoch, loss = load_checkpoint('PyTorch_best_model.pt')"
      ],
      "metadata": {
        "id": "UQH5EOR-hi1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = best_model.to(device)\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = best_model(X_test).detach()\n",
        "    if next(best_model.parameters()).is_cuda:\n",
        "        y_pred = y_pred.cpu()\n",
        "    y_pred = y_pred.numpy()\n",
        "if next(best_model.parameters()).is_cuda:\n",
        "    y_test = y_test.cpu()\n",
        "print(f'Test Score: {round(roc_auc_score(y_test, y_pred), 4)}')"
      ],
      "metadata": {
        "id": "qhnXg7BLGKIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto terminate session\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "yR-W2KjJJ5Zh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}